# Ajuste dos pesos com backpropagation

O algoritmo é chamado de **backpropagation** justamente porque ele faz os
cálculos e vai atualizando a camada de saída até ele chegar lá na camada de
entrada.

## resumo do cálculo de atualização dos pesos

Vamos efetivamente fazer a atualização dos pesos com essa formula:
**peso n + 1** peso na próxima interação vai receber **peso n** (peso atual)
multiplicado pelo **momento** (parâmetro que acelera a decida do gradiente, ele
é muito utilizado quando você utiliza a descida do gradiente _estocástica_.) e
nós fazemos o somatório pela pela **entrada** atual multiplicado pelo **delta** e multiplicado pela **taxa de aprendizagem**.

![alt text](../imagens/backpropagation/ex1.png)

observação: Ao utilizarmos o otimizador **ADAM** que é uma melhoria na descida do
gradiente tradicional, ele não utiliza o parâmetro **momento**

## Ajuste dos pesos da camada oculta para a camada de saída

Esse tipo de rede neural que estamos trabalhando é chamada de **feedforward**
ou seja primeiro é feito o processo para frente até chegar no valor de saída.
Depois que ele faz o cálculo do erro ele volta para fazer a atualização dos
pesos.

![alt text](../imagens/backpropagation/ex2.png)

Para o ajuste dos pesos da camada oculta faremos somente o cálculo da
**entrada + delta de saída**.

![alt text](../imagens/backpropagation/ex3.png)

Após esse calculo vamos fazer a utilização da fórmula que vai efetivamente atualizar os pesos,
vamos considerar o valor da **taxa de aprendizagem** = 0.3.
Na maioria das bibliotecas vamos ter esse valor como 0.0001.

![alt text](../imagens/backpropagation/ex4.png)

## Ajuste dos pesos da camada de entrada para a camada oculta

Faremos o calculo de somar os valores de **entrada + delta**.
![alt text](../imagens/backpropagation/ex5.png)

Os pesos atualizados ficam assim:
![alt text](../imagens/backpropagation/ex6.png)

Cada vez que ele vai fazendo a atualização dos pesos isso é chamado de **épocas**.
